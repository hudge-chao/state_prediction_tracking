{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch \n",
    "from ConvAE import Encoder, Decoder\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import tensorboardX\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE_Dataset(Dataset):\n",
    "    def __init__(self, dir=\"./maps\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataset_dir = dir\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        files = os.listdir(self.dataset_dir)\n",
    "        return len(files)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = \"{}.png\".format(index)\n",
    "        image_path = os.path.join(self.dataset_dir, image_name)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = image / 255.0\n",
    "        image = np.expand_dims(image, 0)\n",
    "        return image, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集样本数量:  12151\n",
      "图片尺寸:  (1, 300, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_conv): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (10): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): ReLU(inplace)\n",
       "    (16): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (20): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace)\n",
       "    (23): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (24): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace)\n",
       "    (26): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (28): ReLU(inplace)\n",
       "    (29): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (30): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace)\n",
       "    (33): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (34): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace)\n",
       "    (36): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (37): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace)\n",
       "    (39): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (40): ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace)\n",
       "    (43): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (44): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataset = ConvAE_Dataset(\"./localmaps\")\n",
    "\n",
    "train_dataset_size = int(len(myDataset) * 0.8)\n",
    "test_dataset_size = len(myDataset) - train_dataset_size\n",
    "# validate_dataset_size = len(myDataset) - (train_dataset_size + test_dataset_size)\n",
    "\n",
    "# train_dataset, test_dataset, validate_dataset = random_split(myDataset, [train_dataset_size, test_dataset_size, validate_dataset_size])\n",
    "train_dataset, test_dataset = random_split(myDataset, [train_dataset_size, test_dataset_size])\n",
    "\n",
    "# train_dataset_loader = DataLoader(train_dataset, batch_size=20, shuffle=False) \n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, batch_size=20, shuffle=False) \n",
    "test_dataset_loader = DataLoader(test_dataset, batch_size=20, shuffle=False) \n",
    "# validate_dataset_loader = DataLoader(validate_dataset, batch_size=20, shuffle=False) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print('数据集样本数量: ', len(myDataset))\n",
    "\n",
    "print('图片尺寸: ', myDataset[0][0].shape)\n",
    "\n",
    "writer = tensorboardX.SummaryWriter('./log', flush_secs=2)\n",
    "\n",
    "train_epoches = 500\n",
    "\n",
    "evaluate_frequency = 10\n",
    "\n",
    "test_frequency = 5\n",
    "\n",
    "save_weight_frequency = 50\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.Softmax2d()\n",
    "\n",
    "lr = 0.005\n",
    " \n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, train loss: 214.7642970085144\n",
      "train epoch: 2, train loss: 182.89212882518768\n",
      "train epoch: 3, train loss: 166.73746705055237\n",
      "train epoch: 4, train loss: 157.33860433101654\n",
      "train epoch: 5, train loss: 152.11106836795807\n",
      "train epoch: 6, train loss: 148.54562282562256\n",
      "------test------\n",
      "train epoch: 7, train loss: 153.84329855442047\n",
      "train epoch: 8, train loss: 146.50002121925354\n",
      "train epoch: 9, train loss: 182.49157071113586\n",
      "train epoch: 10, train loss: 159.46637094020844\n",
      "train epoch: 11, train loss: 152.3323655128479\n",
      "test epoch: 1.0, test loss: 150.4339575767517\n",
      "------test------\n",
      "train epoch: 12, train loss: 146.98271453380585\n",
      "train epoch: 13, train loss: 144.49670910835266\n",
      "train epoch: 14, train loss: 144.9524611234665\n",
      "train epoch: 15, train loss: 147.21128344535828\n",
      "train epoch: 16, train loss: 161.59208118915558\n",
      "------test------\n",
      "train epoch: 17, train loss: 146.39759063720703\n",
      "train epoch: 18, train loss: 141.14372432231903\n",
      "train epoch: 19, train loss: 138.189435005188\n",
      "train epoch: 20, train loss: 136.871799826622\n",
      "train epoch: 21, train loss: 181.7043274641037\n",
      "test epoch: 2.0, test loss: 192.37251579761505\n",
      "------test------\n",
      "train epoch: 22, train loss: 177.43833363056183\n",
      "train epoch: 23, train loss: 163.14157843589783\n",
      "train epoch: 24, train loss: 155.05476295948029\n",
      "train epoch: 25, train loss: 150.37190914154053\n",
      "train epoch: 26, train loss: 146.75875008106232\n",
      "------test------\n",
      "train epoch: 27, train loss: 143.91212165355682\n",
      "train epoch: 28, train loss: 141.49105548858643\n",
      "train epoch: 29, train loss: 139.80896770954132\n",
      "train epoch: 30, train loss: 138.83590698242188\n",
      "train epoch: 31, train loss: 137.07761466503143\n",
      "test epoch: 3.0, test loss: 138.5592371225357\n",
      "------test------\n",
      "train epoch: 32, train loss: 136.0579878091812\n",
      "train epoch: 33, train loss: 135.01332700252533\n",
      "train epoch: 34, train loss: 134.45959985256195\n",
      "train epoch: 35, train loss: 133.56582820415497\n",
      "train epoch: 36, train loss: 133.16453993320465\n",
      "------test------\n",
      "train epoch: 37, train loss: 132.472425699234\n",
      "train epoch: 38, train loss: 131.89774751663208\n",
      "train epoch: 39, train loss: 131.41867518424988\n",
      "train epoch: 40, train loss: 131.21283054351807\n",
      "train epoch: 41, train loss: 130.8368593454361\n",
      "test epoch: 4.0, test loss: 131.4956545829773\n",
      "------test------\n",
      "train epoch: 42, train loss: 130.41168451309204\n",
      "train epoch: 43, train loss: 130.03455102443695\n",
      "train epoch: 44, train loss: 129.98738884925842\n",
      "train epoch: 45, train loss: 129.63879108428955\n",
      "train epoch: 46, train loss: 129.58790361881256\n",
      "------test------\n",
      "train epoch: 47, train loss: 129.33945655822754\n",
      "train epoch: 48, train loss: 129.1891634464264\n",
      "train epoch: 49, train loss: 128.99483740329742\n",
      "train epoch: 50, train loss: 128.60223650932312\n",
      "train epoch: 51, train loss: 128.61871719360352\n",
      "test epoch: 5.0, test loss: 131.87555968761444\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 52, train loss: 128.4485012292862\n",
      "train epoch: 53, train loss: 128.29488515853882\n",
      "train epoch: 54, train loss: 128.05859744548798\n",
      "train epoch: 55, train loss: 128.14368307590485\n",
      "train epoch: 56, train loss: 128.01796197891235\n",
      "------test------\n",
      "train epoch: 57, train loss: 127.672478556633\n",
      "train epoch: 58, train loss: 127.78574228286743\n",
      "train epoch: 59, train loss: 127.45554745197296\n",
      "train epoch: 60, train loss: 127.29215621948242\n",
      "train epoch: 61, train loss: 127.12602317333221\n",
      "test epoch: 6.0, test loss: 133.39364528656006\n",
      "------test------\n",
      "train epoch: 62, train loss: 127.33007967472076\n",
      "train epoch: 63, train loss: 126.90931558609009\n",
      "train epoch: 64, train loss: 127.45188176631927\n",
      "train epoch: 65, train loss: 127.14642286300659\n",
      "train epoch: 66, train loss: 126.78709626197815\n",
      "------test------\n",
      "train epoch: 67, train loss: 127.2662878036499\n",
      "train epoch: 68, train loss: 126.75003707408905\n",
      "train epoch: 69, train loss: 126.72463059425354\n",
      "train epoch: 70, train loss: 126.67803466320038\n",
      "train epoch: 71, train loss: 126.29298865795135\n",
      "test epoch: 7.0, test loss: 134.33538377285004\n",
      "------test------\n",
      "train epoch: 72, train loss: 126.7995685338974\n",
      "train epoch: 73, train loss: 126.74982845783234\n",
      "train epoch: 74, train loss: 126.68859958648682\n",
      "train epoch: 75, train loss: 126.09158456325531\n",
      "train epoch: 76, train loss: 126.36607885360718\n",
      "------test------\n",
      "train epoch: 77, train loss: 126.24256312847137\n",
      "train epoch: 78, train loss: 126.41678750514984\n",
      "train epoch: 79, train loss: 126.3582855463028\n",
      "train epoch: 80, train loss: 126.02286040782928\n",
      "train epoch: 81, train loss: 125.90137124061584\n",
      "test epoch: 8.0, test loss: 127.28908658027649\n",
      "------test------\n",
      "train epoch: 82, train loss: 126.04352831840515\n",
      "train epoch: 83, train loss: 126.14558637142181\n",
      "train epoch: 84, train loss: 125.69040060043335\n",
      "train epoch: 85, train loss: 126.18379294872284\n",
      "train epoch: 86, train loss: 126.04446709156036\n",
      "------test------\n",
      "train epoch: 87, train loss: 125.7537454366684\n",
      "train epoch: 88, train loss: 126.16337835788727\n",
      "train epoch: 89, train loss: 125.74830651283264\n",
      "train epoch: 90, train loss: 125.44745206832886\n",
      "train epoch: 91, train loss: 125.1453161239624\n",
      "test epoch: 9.0, test loss: 129.01832163333893\n",
      "------test------\n",
      "train epoch: 92, train loss: 125.34348666667938\n",
      "train epoch: 93, train loss: 125.43603777885437\n",
      "train epoch: 94, train loss: 125.25971233844757\n",
      "train epoch: 95, train loss: 125.17265975475311\n",
      "train epoch: 96, train loss: 125.31870603561401\n",
      "------test------\n",
      "train epoch: 97, train loss: 125.48844516277313\n",
      "train epoch: 98, train loss: 125.45402348041534\n",
      "train epoch: 99, train loss: 125.07669627666473\n",
      "train epoch: 100, train loss: 125.10339915752411\n",
      "train epoch: 101, train loss: 125.1363456249237\n",
      "test epoch: 10.0, test loss: 131.00619614124298\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 102, train loss: 125.13163685798645\n",
      "train epoch: 103, train loss: 125.19742548465729\n",
      "train epoch: 104, train loss: 125.30563771724701\n",
      "train epoch: 105, train loss: 125.02488493919373\n",
      "train epoch: 106, train loss: 124.94438141584396\n",
      "------test------\n",
      "train epoch: 107, train loss: 125.13601779937744\n",
      "train epoch: 108, train loss: 125.01104176044464\n",
      "train epoch: 109, train loss: 124.99427050352097\n",
      "train epoch: 110, train loss: 125.10669231414795\n",
      "train epoch: 111, train loss: 124.83014911413193\n",
      "test epoch: 11.0, test loss: 129.7556459903717\n",
      "------test------\n",
      "train epoch: 112, train loss: 124.90160018205643\n",
      "train epoch: 113, train loss: 124.8961091041565\n",
      "train epoch: 114, train loss: 125.0157505273819\n",
      "train epoch: 115, train loss: 124.9351054430008\n",
      "train epoch: 116, train loss: 124.70106780529022\n",
      "------test------\n",
      "train epoch: 117, train loss: 124.59101527929306\n",
      "train epoch: 118, train loss: 124.98000264167786\n",
      "train epoch: 119, train loss: 124.75759536027908\n",
      "train epoch: 120, train loss: 124.53047186136246\n",
      "train epoch: 121, train loss: 124.98482316732407\n",
      "test epoch: 12.0, test loss: 128.5434365272522\n",
      "------test------\n",
      "train epoch: 122, train loss: 124.80276823043823\n",
      "train epoch: 123, train loss: 124.44471567869186\n",
      "train epoch: 124, train loss: 124.80733543634415\n",
      "train epoch: 125, train loss: 124.82281029224396\n",
      "train epoch: 126, train loss: 125.01022219657898\n",
      "------test------\n",
      "train epoch: 127, train loss: 124.56004321575165\n",
      "train epoch: 128, train loss: 124.47091192007065\n",
      "train epoch: 129, train loss: 124.63061511516571\n",
      "train epoch: 130, train loss: 124.53766167163849\n",
      "train epoch: 131, train loss: 124.4351863861084\n",
      "test epoch: 13.0, test loss: 126.13226473331451\n",
      "------test------\n",
      "train epoch: 132, train loss: 124.71585720777512\n",
      "train epoch: 133, train loss: 124.52854961156845\n",
      "train epoch: 134, train loss: 124.46518987417221\n",
      "train epoch: 135, train loss: 124.6173232793808\n",
      "train epoch: 136, train loss: 124.68410283327103\n",
      "------test------\n",
      "train epoch: 137, train loss: 124.38981980085373\n",
      "train epoch: 138, train loss: 124.45805221796036\n",
      "train epoch: 139, train loss: 124.6054470539093\n",
      "train epoch: 140, train loss: 124.24176931381226\n",
      "train epoch: 141, train loss: 124.85119700431824\n",
      "test epoch: 14.0, test loss: 126.73263251781464\n",
      "------test------\n",
      "train epoch: 142, train loss: 124.29835647344589\n",
      "train epoch: 143, train loss: 124.58056211471558\n",
      "train epoch: 144, train loss: 124.18407946825027\n",
      "train epoch: 145, train loss: 124.11968410015106\n",
      "train epoch: 146, train loss: 124.45777654647827\n",
      "------test------\n",
      "train epoch: 147, train loss: 124.4155764579773\n",
      "train epoch: 148, train loss: 124.08362329006195\n",
      "train epoch: 149, train loss: 124.15345758199692\n",
      "train epoch: 150, train loss: 124.15767461061478\n",
      "train epoch: 151, train loss: 124.01308119297028\n",
      "test epoch: 15.0, test loss: 126.53867900371552\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 152, train loss: 123.90103191137314\n",
      "train epoch: 153, train loss: 124.33882802724838\n",
      "train epoch: 154, train loss: 124.30485337972641\n",
      "train epoch: 155, train loss: 124.08564984798431\n",
      "train epoch: 156, train loss: 123.8168552517891\n",
      "------test------\n",
      "train epoch: 157, train loss: 124.14789944887161\n",
      "train epoch: 158, train loss: 124.0013837814331\n",
      "train epoch: 159, train loss: 124.37298893928528\n",
      "train epoch: 160, train loss: 123.82091581821442\n",
      "train epoch: 161, train loss: 123.89140576124191\n",
      "test epoch: 16.0, test loss: 125.60833990573883\n",
      "------test------\n",
      "train epoch: 162, train loss: 123.76633286476135\n",
      "train epoch: 163, train loss: 124.0113377571106\n",
      "train epoch: 164, train loss: 124.08476322889328\n",
      "train epoch: 165, train loss: 124.44035708904266\n",
      "train epoch: 166, train loss: 124.12849813699722\n",
      "------test------\n",
      "train epoch: 167, train loss: 124.2688000202179\n",
      "train epoch: 168, train loss: 124.00493025779724\n",
      "train epoch: 169, train loss: 123.98558855056763\n",
      "train epoch: 170, train loss: 123.86207282543182\n",
      "train epoch: 171, train loss: 124.18423593044281\n",
      "test epoch: 17.0, test loss: 126.9550770521164\n",
      "------test------\n",
      "train epoch: 172, train loss: 123.82306158542633\n",
      "train epoch: 173, train loss: 123.54455888271332\n",
      "train epoch: 174, train loss: 124.01009351015091\n",
      "train epoch: 175, train loss: 123.8858699798584\n",
      "train epoch: 176, train loss: 123.96810203790665\n",
      "------test------\n",
      "train epoch: 177, train loss: 123.83874505758286\n",
      "train epoch: 178, train loss: 124.14650619029999\n",
      "train epoch: 179, train loss: 123.74574691057205\n",
      "train epoch: 180, train loss: 124.17147308588028\n",
      "train epoch: 181, train loss: 123.66387993097305\n",
      "test epoch: 18.0, test loss: 125.4214197397232\n",
      "------test------\n",
      "train epoch: 182, train loss: 123.73058497905731\n",
      "train epoch: 183, train loss: 124.12498146295547\n",
      "train epoch: 184, train loss: 123.8270103931427\n",
      "train epoch: 185, train loss: 123.96326661109924\n",
      "train epoch: 186, train loss: 124.01759624481201\n",
      "------test------\n",
      "train epoch: 187, train loss: 123.61759692430496\n",
      "train epoch: 188, train loss: 123.83809685707092\n",
      "train epoch: 189, train loss: 123.87659400701523\n",
      "train epoch: 190, train loss: 123.81995469331741\n",
      "train epoch: 191, train loss: 123.55291843414307\n",
      "test epoch: 19.0, test loss: 125.46464800834656\n",
      "------test------\n",
      "train epoch: 192, train loss: 124.27105754613876\n",
      "train epoch: 193, train loss: 123.57320636510849\n",
      "train epoch: 194, train loss: 123.7802654504776\n",
      "train epoch: 195, train loss: 123.55628609657288\n",
      "train epoch: 196, train loss: 124.10665303468704\n",
      "------test------\n",
      "train epoch: 197, train loss: 124.06273186206818\n",
      "train epoch: 198, train loss: 123.70678037405014\n",
      "train epoch: 199, train loss: 123.70122969150543\n",
      "train epoch: 200, train loss: 123.79913032054901\n",
      "train epoch: 201, train loss: 123.50616604089737\n",
      "test epoch: 20.0, test loss: 127.4312287569046\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 202, train loss: 123.98012727499008\n",
      "train epoch: 203, train loss: 123.86220693588257\n",
      "train epoch: 204, train loss: 123.84362518787384\n",
      "train epoch: 205, train loss: 123.94466996192932\n",
      "train epoch: 206, train loss: 123.29437583684921\n",
      "------test------\n",
      "train epoch: 207, train loss: 123.71523678302765\n",
      "train epoch: 208, train loss: 123.42501431703568\n",
      "train epoch: 209, train loss: 123.71240556240082\n",
      "train epoch: 210, train loss: 123.991958796978\n",
      "train epoch: 211, train loss: 124.11309778690338\n",
      "test epoch: 21.0, test loss: 125.7656067609787\n",
      "------test------\n",
      "train epoch: 212, train loss: 123.94257634878159\n",
      "train epoch: 213, train loss: 124.03927743434906\n",
      "train epoch: 214, train loss: 123.30491095781326\n",
      "train epoch: 215, train loss: 123.32078069448471\n",
      "train epoch: 216, train loss: 123.71087074279785\n",
      "------test------\n",
      "train epoch: 217, train loss: 123.73650074005127\n",
      "train epoch: 218, train loss: 123.49703907966614\n",
      "train epoch: 219, train loss: 123.58766794204712\n",
      "train epoch: 220, train loss: 123.75053018331528\n",
      "train epoch: 221, train loss: 123.56758117675781\n",
      "test epoch: 22.0, test loss: 130.05168735980988\n",
      "------test------\n",
      "train epoch: 222, train loss: 123.66132438182831\n",
      "train epoch: 223, train loss: 123.69919568300247\n",
      "train epoch: 224, train loss: 123.28369915485382\n",
      "train epoch: 225, train loss: 123.44405800104141\n",
      "train epoch: 226, train loss: 123.62920492887497\n",
      "------test------\n",
      "train epoch: 227, train loss: 123.27226996421814\n",
      "train epoch: 228, train loss: 123.41553717851639\n",
      "train epoch: 229, train loss: 123.2711598277092\n",
      "train epoch: 230, train loss: 123.59946966171265\n",
      "train epoch: 231, train loss: 123.53163957595825\n",
      "test epoch: 23.0, test loss: 124.66131895780563\n",
      "------test------\n",
      "train epoch: 232, train loss: 123.25241416692734\n",
      "train epoch: 233, train loss: 123.2655867934227\n",
      "train epoch: 234, train loss: 123.63915145397186\n",
      "train epoch: 235, train loss: 123.78943711519241\n",
      "train epoch: 236, train loss: 123.72686713933945\n",
      "------test------\n",
      "train epoch: 237, train loss: 123.44939261674881\n",
      "train epoch: 238, train loss: 123.1929287314415\n",
      "train epoch: 239, train loss: 123.28304350376129\n",
      "train epoch: 240, train loss: 123.28572571277618\n",
      "train epoch: 241, train loss: 123.38607758283615\n",
      "test epoch: 24.0, test loss: 125.18724799156189\n",
      "------test------\n",
      "train epoch: 242, train loss: 123.61887842416763\n",
      "train epoch: 243, train loss: 123.78980219364166\n",
      "train epoch: 244, train loss: 123.16020578145981\n",
      "train epoch: 245, train loss: 123.44245612621307\n",
      "train epoch: 246, train loss: 123.47928434610367\n",
      "------test------\n",
      "train epoch: 247, train loss: 123.03313612937927\n",
      "train epoch: 248, train loss: 123.39508533477783\n",
      "train epoch: 249, train loss: 123.22687357664108\n",
      "train epoch: 250, train loss: 123.56303632259369\n",
      "train epoch: 251, train loss: 123.23778122663498\n",
      "test epoch: 25.0, test loss: 128.41936945915222\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 252, train loss: 123.33232164382935\n",
      "train epoch: 253, train loss: 123.44314903020859\n",
      "train epoch: 254, train loss: 123.17172437906265\n",
      "train epoch: 255, train loss: 123.59300255775452\n",
      "train epoch: 256, train loss: 123.24929237365723\n",
      "------test------\n",
      "train epoch: 257, train loss: 123.28369170427322\n",
      "train epoch: 258, train loss: 123.14169853925705\n",
      "train epoch: 259, train loss: 123.46156686544418\n",
      "train epoch: 260, train loss: 123.39621037244797\n",
      "train epoch: 261, train loss: 122.96789884567261\n",
      "test epoch: 26.0, test loss: 125.30727684497833\n",
      "------test------\n",
      "train epoch: 262, train loss: 123.37909638881683\n",
      "train epoch: 263, train loss: 123.02517145872116\n",
      "train epoch: 264, train loss: 123.2767254114151\n",
      "train epoch: 265, train loss: 123.44393134117126\n",
      "train epoch: 266, train loss: 123.09656292200089\n",
      "------test------\n",
      "train epoch: 267, train loss: 123.3411654829979\n",
      "train epoch: 268, train loss: 123.10965359210968\n",
      "train epoch: 269, train loss: 123.05551022291183\n",
      "train epoch: 270, train loss: 122.84791469573975\n",
      "train epoch: 271, train loss: 123.14605712890625\n",
      "test epoch: 27.0, test loss: 126.60787999629974\n",
      "------test------\n",
      "train epoch: 272, train loss: 123.44755977392197\n",
      "train epoch: 273, train loss: 123.39328974485397\n",
      "train epoch: 274, train loss: 123.13640862703323\n",
      "train epoch: 275, train loss: 123.21104854345322\n",
      "train epoch: 276, train loss: 123.23138117790222\n",
      "------test------\n",
      "train epoch: 277, train loss: 123.06956201791763\n",
      "train epoch: 278, train loss: 123.10436367988586\n",
      "train epoch: 279, train loss: 123.46264719963074\n",
      "train epoch: 280, train loss: 123.3675628900528\n",
      "train epoch: 281, train loss: 122.88221716880798\n",
      "test epoch: 28.0, test loss: 126.49677693843842\n",
      "------test------\n",
      "train epoch: 282, train loss: 123.00781905651093\n",
      "train epoch: 283, train loss: 123.29422682523727\n",
      "train epoch: 284, train loss: 123.26914072036743\n",
      "train epoch: 285, train loss: 123.18597733974457\n",
      "train epoch: 286, train loss: 123.19338321685791\n",
      "------test------\n",
      "train epoch: 287, train loss: 123.58298897743225\n",
      "train epoch: 288, train loss: 122.77121096849442\n",
      "train epoch: 289, train loss: 123.13239276409149\n",
      "train epoch: 290, train loss: 122.92638421058655\n",
      "train epoch: 291, train loss: 123.15421551465988\n",
      "test epoch: 29.0, test loss: 126.15610659122467\n",
      "------test------\n",
      "train epoch: 292, train loss: 123.13827127218246\n",
      "train epoch: 293, train loss: 123.56051057577133\n",
      "train epoch: 294, train loss: 123.52108210325241\n",
      "train epoch: 295, train loss: 122.89086729288101\n",
      "train epoch: 296, train loss: 122.837133705616\n",
      "------test------\n",
      "train epoch: 297, train loss: 122.77240306138992\n",
      "train epoch: 298, train loss: 123.22525680065155\n",
      "train epoch: 299, train loss: 123.03691357374191\n",
      "train epoch: 300, train loss: 123.14154952764511\n",
      "train epoch: 301, train loss: 123.24386835098267\n",
      "test epoch: 30.0, test loss: 126.6586035490036\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 302, train loss: 123.08431416749954\n",
      "train epoch: 303, train loss: 122.88068234920502\n",
      "train epoch: 304, train loss: 123.2413798570633\n",
      "train epoch: 305, train loss: 122.8630468249321\n",
      "train epoch: 306, train loss: 122.8916123509407\n",
      "------test------\n",
      "train epoch: 307, train loss: 123.04119020700455\n",
      "train epoch: 308, train loss: 122.8775829076767\n",
      "train epoch: 309, train loss: 122.92786687612534\n",
      "train epoch: 310, train loss: 123.10007959604263\n",
      "train epoch: 311, train loss: 122.65036255121231\n",
      "test epoch: 31.0, test loss: 125.85438787937164\n",
      "------test------\n",
      "train epoch: 312, train loss: 122.92714416980743\n",
      "train epoch: 313, train loss: 123.51352721452713\n",
      "train epoch: 314, train loss: 122.74087220430374\n",
      "train epoch: 315, train loss: 122.84290045499802\n",
      "train epoch: 316, train loss: 123.05033206939697\n",
      "------test------\n",
      "train epoch: 317, train loss: 122.86709994077682\n",
      "train epoch: 318, train loss: 122.8414997458458\n",
      "train epoch: 319, train loss: 122.65263497829437\n",
      "train epoch: 320, train loss: 122.7126345038414\n",
      "train epoch: 321, train loss: 122.75288999080658\n",
      "test epoch: 32.0, test loss: 125.3451406955719\n",
      "------test------\n",
      "train epoch: 322, train loss: 122.9267418384552\n",
      "train epoch: 323, train loss: 122.89823591709137\n",
      "train epoch: 324, train loss: 123.02379310131073\n",
      "train epoch: 325, train loss: 122.86923080682755\n",
      "train epoch: 326, train loss: 122.61521816253662\n",
      "------test------\n",
      "train epoch: 327, train loss: 123.03747236728668\n",
      "train epoch: 328, train loss: 122.88444489240646\n",
      "train epoch: 329, train loss: 122.96312302350998\n",
      "train epoch: 330, train loss: 122.75974452495575\n",
      "train epoch: 331, train loss: 122.74611741304398\n",
      "test epoch: 33.0, test loss: 123.96381050348282\n",
      "------test------\n",
      "train epoch: 332, train loss: 123.09500575065613\n",
      "train epoch: 333, train loss: 123.14532697200775\n",
      "train epoch: 334, train loss: 122.61582165956497\n",
      "train epoch: 335, train loss: 122.82190471887589\n",
      "train epoch: 336, train loss: 122.98368662595749\n",
      "------test------\n",
      "train epoch: 337, train loss: 122.72460013628006\n",
      "train epoch: 338, train loss: 122.79380857944489\n",
      "train epoch: 339, train loss: 122.69941717386246\n",
      "train epoch: 340, train loss: 122.7974221110344\n",
      "train epoch: 341, train loss: 122.80077487230301\n",
      "test epoch: 34.0, test loss: 125.65726041793823\n",
      "------test------\n",
      "train epoch: 342, train loss: 122.56062030792236\n",
      "train epoch: 343, train loss: 123.02275002002716\n",
      "train epoch: 344, train loss: 122.82425165176392\n",
      "train epoch: 345, train loss: 122.6191595196724\n",
      "train epoch: 346, train loss: 122.9400709271431\n",
      "------test------\n",
      "train epoch: 347, train loss: 123.12228977680206\n",
      "train epoch: 348, train loss: 123.19844216108322\n",
      "train epoch: 349, train loss: 122.70531058311462\n",
      "train epoch: 350, train loss: 122.59702384471893\n",
      "train epoch: 351, train loss: 122.73945659399033\n",
      "test epoch: 35.0, test loss: 125.07729232311249\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 352, train loss: 122.59238958358765\n",
      "train epoch: 353, train loss: 123.13902378082275\n",
      "train epoch: 354, train loss: 123.11536818742752\n",
      "train epoch: 355, train loss: 123.16381931304932\n",
      "train epoch: 356, train loss: 122.74902313947678\n",
      "------test------\n",
      "train epoch: 357, train loss: 122.57975339889526\n",
      "train epoch: 358, train loss: 122.60892987251282\n",
      "train epoch: 359, train loss: 122.72220104932785\n",
      "train epoch: 360, train loss: 123.06276708841324\n",
      "train epoch: 361, train loss: 122.8957325220108\n",
      "test epoch: 36.0, test loss: 125.34201145172119\n",
      "------test------\n",
      "train epoch: 362, train loss: 122.7368488907814\n",
      "train epoch: 363, train loss: 123.29234927892685\n",
      "train epoch: 364, train loss: 122.51168489456177\n",
      "train epoch: 365, train loss: 122.7581799030304\n",
      "train epoch: 366, train loss: 122.99386411905289\n",
      "------test------\n",
      "train epoch: 367, train loss: 122.82763421535492\n",
      "train epoch: 368, train loss: 122.54983186721802\n",
      "train epoch: 369, train loss: 122.67300486564636\n",
      "train epoch: 370, train loss: 122.76683002710342\n",
      "train epoch: 371, train loss: 122.71393090486526\n",
      "test epoch: 37.0, test loss: 125.53620338439941\n",
      "------test------\n",
      "train epoch: 372, train loss: 122.73406982421875\n",
      "train epoch: 373, train loss: 122.50886857509613\n",
      "train epoch: 374, train loss: 122.80334532260895\n",
      "train epoch: 375, train loss: 122.48176336288452\n",
      "train epoch: 376, train loss: 122.73366004228592\n",
      "------test------\n",
      "train epoch: 377, train loss: 122.48347699642181\n",
      "train epoch: 378, train loss: 122.83530831336975\n",
      "train epoch: 379, train loss: 122.65413999557495\n",
      "train epoch: 380, train loss: 122.52616882324219\n",
      "train epoch: 381, train loss: 122.61326611042023\n",
      "test epoch: 38.0, test loss: 124.88476186990738\n",
      "------test------\n",
      "train epoch: 382, train loss: 122.72361665964127\n",
      "train epoch: 383, train loss: 122.81711399555206\n",
      "train epoch: 384, train loss: 122.47563153505325\n",
      "train epoch: 385, train loss: 122.48364835977554\n",
      "train epoch: 386, train loss: 122.52576649188995\n",
      "------test------\n",
      "train epoch: 387, train loss: 122.45813012123108\n",
      "train epoch: 388, train loss: 122.24345654249191\n",
      "train epoch: 389, train loss: 122.50756472349167\n",
      "train epoch: 390, train loss: 122.70822376012802\n",
      "train epoch: 391, train loss: 122.66720831394196\n",
      "test epoch: 39.0, test loss: 125.69844722747803\n",
      "------test------\n",
      "train epoch: 392, train loss: 122.61883169412613\n",
      "train epoch: 393, train loss: 122.7431446313858\n",
      "train epoch: 394, train loss: 123.05095046758652\n",
      "train epoch: 395, train loss: 122.4936917424202\n",
      "train epoch: 396, train loss: 122.70800769329071\n",
      "------test------\n",
      "train epoch: 397, train loss: 122.62166291475296\n",
      "train epoch: 398, train loss: 122.61410057544708\n",
      "train epoch: 399, train loss: 122.84984439611435\n",
      "train epoch: 400, train loss: 122.29687720537186\n",
      "train epoch: 401, train loss: 122.87218868732452\n",
      "test epoch: 40.0, test loss: 125.64001977443695\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 402, train loss: 122.39494919776917\n",
      "train epoch: 403, train loss: 122.78540432453156\n",
      "train epoch: 404, train loss: 122.53406643867493\n",
      "train epoch: 405, train loss: 122.20308184623718\n",
      "train epoch: 406, train loss: 122.61731922626495\n",
      "------test------\n",
      "train epoch: 407, train loss: 122.3929300904274\n",
      "train epoch: 408, train loss: 122.73035943508148\n",
      "train epoch: 409, train loss: 122.40193784236908\n",
      "train epoch: 410, train loss: 122.63959646224976\n",
      "train epoch: 411, train loss: 122.70543724298477\n",
      "test epoch: 41.0, test loss: 125.84203481674194\n",
      "------test------\n",
      "train epoch: 412, train loss: 122.78582900762558\n",
      "train epoch: 413, train loss: 122.67318367958069\n",
      "train epoch: 414, train loss: 123.05335700511932\n",
      "train epoch: 415, train loss: 122.32526391744614\n",
      "train epoch: 416, train loss: 122.44195491075516\n",
      "------test------\n",
      "train epoch: 417, train loss: 122.44033813476562\n",
      "train epoch: 418, train loss: 122.77810275554657\n",
      "train epoch: 419, train loss: 122.350312769413\n",
      "train epoch: 420, train loss: 122.61933833360672\n",
      "train epoch: 421, train loss: 123.34975600242615\n",
      "test epoch: 42.0, test loss: 127.74811685085297\n",
      "------test------\n",
      "train epoch: 422, train loss: 122.50466644763947\n",
      "train epoch: 423, train loss: 122.74473905563354\n",
      "train epoch: 424, train loss: 122.57231771945953\n",
      "train epoch: 425, train loss: 122.47879058122635\n",
      "train epoch: 426, train loss: 122.6939707994461\n",
      "------test------\n",
      "train epoch: 427, train loss: 122.34719097614288\n",
      "train epoch: 428, train loss: 122.51637876033783\n",
      "train epoch: 429, train loss: 122.42594361305237\n",
      "train epoch: 430, train loss: 122.85157293081284\n",
      "train epoch: 431, train loss: 122.75700271129608\n",
      "test epoch: 43.0, test loss: 123.03096801042557\n",
      "------test------\n",
      "train epoch: 432, train loss: 122.4527582526207\n",
      "train epoch: 433, train loss: 122.41492420434952\n",
      "train epoch: 434, train loss: 122.39919602870941\n",
      "train epoch: 435, train loss: 122.5651428103447\n",
      "train epoch: 436, train loss: 122.65710532665253\n",
      "------test------\n",
      "train epoch: 437, train loss: 122.78437614440918\n",
      "train epoch: 438, train loss: 123.0243593454361\n",
      "train epoch: 439, train loss: 122.79348820447922\n",
      "train epoch: 440, train loss: 122.45453149080276\n",
      "train epoch: 441, train loss: 122.31503427028656\n",
      "test epoch: 44.0, test loss: 123.94935637712479\n",
      "------test------\n",
      "train epoch: 442, train loss: 122.47350811958313\n",
      "train epoch: 443, train loss: 122.74094671010971\n",
      "train epoch: 444, train loss: 122.6091980934143\n",
      "train epoch: 445, train loss: 122.60611355304718\n",
      "train epoch: 446, train loss: 122.65089154243469\n",
      "------test------\n",
      "train epoch: 447, train loss: 122.47566878795624\n",
      "train epoch: 448, train loss: 122.64072895050049\n",
      "train epoch: 449, train loss: 122.22104519605637\n",
      "train epoch: 450, train loss: 122.81732261180878\n",
      "train epoch: 451, train loss: 123.08533489704132\n",
      "test epoch: 45.0, test loss: 127.1401196718216\n",
      "------test------\n",
      "======= saving models =======\n",
      "train epoch: 452, train loss: 122.38775193691254\n",
      "train epoch: 453, train loss: 122.37656861543655\n",
      "train epoch: 454, train loss: 122.68976867198944\n",
      "train epoch: 455, train loss: 122.59171903133392\n",
      "train epoch: 456, train loss: 122.3330944776535\n",
      "------test------\n",
      "train epoch: 457, train loss: 122.22041934728622\n",
      "train epoch: 458, train loss: 122.55436927080154\n",
      "train epoch: 459, train loss: 122.6673498749733\n",
      "train epoch: 460, train loss: 122.2834512591362\n",
      "train epoch: 461, train loss: 122.89703637361526\n",
      "test epoch: 46.0, test loss: 125.34020841121674\n",
      "------test------\n",
      "train epoch: 462, train loss: 122.34301120042801\n",
      "train epoch: 463, train loss: 122.56172299385071\n",
      "train epoch: 464, train loss: 122.22927063703537\n",
      "train epoch: 465, train loss: 122.38849699497223\n",
      "train epoch: 466, train loss: 122.46901541948318\n",
      "------test------\n",
      "train epoch: 467, train loss: 122.33925610780716\n",
      "train epoch: 468, train loss: 122.51853197813034\n",
      "train epoch: 469, train loss: 122.38675355911255\n",
      "train epoch: 470, train loss: 122.44105339050293\n",
      "train epoch: 471, train loss: 122.61995673179626\n",
      "test epoch: 47.0, test loss: 127.95376777648926\n",
      "------test------\n",
      "train epoch: 472, train loss: 122.82202392816544\n",
      "train epoch: 473, train loss: 121.92653864622116\n",
      "train epoch: 474, train loss: 122.21018224954605\n",
      "train epoch: 475, train loss: 122.21310287714005\n",
      "train epoch: 476, train loss: 122.58914113044739\n",
      "------test------\n",
      "train epoch: 477, train loss: 122.22731113433838\n",
      "train epoch: 478, train loss: 122.67830967903137\n",
      "train epoch: 479, train loss: 122.29414284229279\n",
      "train epoch: 480, train loss: 122.37456440925598\n",
      "train epoch: 481, train loss: 122.54606932401657\n",
      "test epoch: 48.0, test loss: 127.11641192436218\n",
      "------test------\n",
      "train epoch: 482, train loss: 122.37183004617691\n",
      "train epoch: 483, train loss: 122.49751389026642\n",
      "train epoch: 484, train loss: 122.15439230203629\n",
      "train epoch: 485, train loss: 122.63793498277664\n",
      "train epoch: 486, train loss: 122.64277040958405\n",
      "------test------\n",
      "train epoch: 487, train loss: 122.41800129413605\n",
      "train epoch: 488, train loss: 122.48935550451279\n",
      "train epoch: 489, train loss: 122.19122052192688\n",
      "train epoch: 490, train loss: 122.45204299688339\n",
      "train epoch: 491, train loss: 122.33104556798935\n",
      "test epoch: 49.0, test loss: 125.09840726852417\n",
      "------test------\n",
      "train epoch: 492, train loss: 122.39043414592743\n",
      "train epoch: 493, train loss: 122.38678336143494\n",
      "train epoch: 494, train loss: 122.52101302146912\n",
      "train epoch: 495, train loss: 122.39637970924377\n",
      "train epoch: 496, train loss: 122.72314727306366\n",
      "------test------\n",
      "train epoch: 497, train loss: 122.44293838739395\n",
      "train epoch: 498, train loss: 122.73570150136948\n",
      "train epoch: 499, train loss: 122.46173620223999\n",
      "train epoch: 500, train loss: 122.51415103673935\n"
     ]
    }
   ],
   "source": [
    "for i in range(train_epoches):\n",
    "    train_loss_epoch = []\n",
    "    for image_batch, index_batch in train_dataset_loader:\n",
    "        image_batch_tensor = image_batch.clone().detach().float().to(device)\n",
    "        encoded_data = encoder(image_batch_tensor)\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        loss = torch.sqrt((decoded_data - image_batch_tensor).pow(2).mean())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_numpy = loss.detach().cpu().numpy()\n",
    "        train_loss_epoch.append(loss_numpy)\n",
    "    train_loss_avg = np.mean(train_loss_epoch)\n",
    "    writer.add_scalar('train loss', train_loss_avg * 1000, global_step=i+1)\n",
    "    print(\"train epoch: {}, train loss: {}\".format(i+1, train_loss_avg * 1000))\n",
    "\n",
    "    if (i % evaluate_frequency == 0 and i != 0):\n",
    "        test_loss_epoch = []\n",
    "        for image_test_batch, index_test_batch in test_dataset_loader:\n",
    "            image_test_batch_tensor = image_test_batch.clone().detach().float().to(device)\n",
    "            encoded_test_data = encoder(image_test_batch_tensor)\n",
    "            decoded_test_data = decoder(encoded_test_data)\n",
    "            loss = torch.sqrt((decoded_test_data - image_test_batch_tensor).pow(2).mean())\n",
    "            loss_numpy = loss.detach().cpu().numpy()\n",
    "            test_loss_epoch.append(loss_numpy)\n",
    "        test_loss_avg = np.mean(test_loss_epoch)\n",
    "        writer.add_scalar('test loss', test_loss_avg * 1000, global_step=(i / evaluate_frequency))\n",
    "        print(\"test epoch: {}, test loss: {}\".format(i / evaluate_frequency, test_loss_avg * 1000))\n",
    "\n",
    "    if (i % test_frequency == 0 and i != 0):\n",
    "        print('------test------')\n",
    "        selected_sample_index = random.randint(0, len(myDataset))\n",
    "        selected_sample = myDataset[selected_sample_index][0]\n",
    "        selected_image_origin = np.expand_dims(selected_sample, 0)\n",
    "        # print(selected_image_origin.shape)\n",
    "        selected_image_origin = torch.tensor(selected_image_origin, dtype=torch.float)\n",
    "        selected_image_origin_tensor = selected_image_origin.clone().detach().to(device)\n",
    "        selected_image_infer_tensor = decoder(encoder(selected_image_origin_tensor))\n",
    "        selected_image_infer = selected_image_infer_tensor.detach().cpu().numpy()[0]\n",
    "        selected_image_origin_show =  selected_sample[0] * 255.0\n",
    "        selected_image_infer_show = selected_image_infer[0] * 255.0\n",
    "        image_concat = np.hstack((selected_image_origin_show, selected_image_infer_show))\n",
    "        cv2.imwrite(\"result.png\", image_concat)\n",
    "\n",
    "    if (i % save_weight_frequency == 0 and i != 0):\n",
    "        print('======= saving models =======')\n",
    "        torch.save(encoder.state_dict(), \"./weights/{}_encoder.pth\".format(i))\n",
    "        torch.save(decoder.state_dict(), \"./weights/{}_decoder.pth\".format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ab5697f9e69cd78b920c03fde1b23dee35d76387e0a20d35fdb77766e38d53b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
