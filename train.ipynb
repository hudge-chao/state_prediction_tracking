{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch \n",
    "from ConvAE import Encoder, Decoder\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import tensorboardX\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE_Dataset(Dataset):\n",
    "    def __init__(self, dir=\"./maps\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataset_dir = dir\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        files = os.listdir(self.dataset_dir)\n",
    "        return len(files)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = \"{}.png\".format(index)\n",
    "        image_path = os.path.join(self.dataset_dir, image_name)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = image / 255.0\n",
    "        image = np.expand_dims(image, 0)\n",
    "        return image, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集样本数量:  5424\n",
      "图片尺寸:  (1, 200, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_conv): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace)\n",
       "    (7): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (11): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace)\n",
       "    (14): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (18): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace)\n",
       "    (21): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (22): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace)\n",
       "    (24): ConvTranspose2d(16, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace)\n",
       "    (27): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (28): ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (29): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): ReLU(inplace)\n",
       "    (31): ConvTranspose2d(8, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (32): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (33): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDataset = ConvAE_Dataset(\"./localmaps\")\n",
    "\n",
    "# train_dataset_size = int(len(myDataset) * 0.6)\n",
    "# test_dataset_size = int(len(myDataset) * 0.3)\n",
    "# validate_dataset_size = len(myDataset) - (train_dataset_size + test_dataset_size)\n",
    "\n",
    "# train_dataset, test_dataset, validate_dataset = random_split(myDataset, [train_dataset_size, test_dataset_size, validate_dataset_size])\n",
    "\n",
    "train_dataset_loader = DataLoader(myDataset, batch_size=20, shuffle=False) \n",
    "\n",
    "# train_dataset_loader = DataLoader(train_dataset, batch_size=20, shuffle=False) \n",
    "# test_dataset_loader = DataLoader(test_dataset, batch_size=20, shuffle=False) \n",
    "# validate_dataset_loader = DataLoader(validate_dataset, batch_size=20, shuffle=False) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print('数据集样本数量: ', len(myDataset))\n",
    "\n",
    "print('图片尺寸: ', myDataset[0][0].shape)\n",
    "\n",
    "writer = tensorboardX.SummaryWriter('./log', flush_secs=2)\n",
    "\n",
    "train_epoches = 500\n",
    "\n",
    "test_frequency = 10\n",
    "\n",
    "save_weight_frequency = 100\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "# loss_fn = nn.Softmax2d()\n",
    "\n",
    "lr = 0.005\n",
    " \n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-5)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1, train loss: 249.25260245800018\n",
      "train epoch: 2, train loss: 215.58405458927155\n",
      "train epoch: 3, train loss: 196.9134956598282\n",
      "train epoch: 4, train loss: 191.64380431175232\n",
      "train epoch: 5, train loss: 185.77070534229279\n",
      "train epoch: 6, train loss: 179.85732853412628\n",
      "train epoch: 7, train loss: 176.38416588306427\n",
      "train epoch: 8, train loss: 173.51478338241577\n",
      "train epoch: 9, train loss: 168.60464215278625\n",
      "train epoch: 10, train loss: 168.03908348083496\n",
      "------test------\n",
      "train epoch: 11, train loss: 168.68948936462402\n",
      "train epoch: 12, train loss: 165.63820838928223\n",
      "train epoch: 13, train loss: 167.0791208744049\n",
      "train epoch: 14, train loss: 161.7806851863861\n",
      "train epoch: 15, train loss: 160.65073013305664\n",
      "train epoch: 16, train loss: 161.2980216741562\n",
      "train epoch: 17, train loss: 158.60827267169952\n",
      "train epoch: 18, train loss: 160.23363173007965\n",
      "train epoch: 19, train loss: 157.59053826332092\n",
      "train epoch: 20, train loss: 156.64660930633545\n",
      "------test------\n",
      "train epoch: 21, train loss: 155.40046989917755\n",
      "train epoch: 22, train loss: 155.00767529010773\n",
      "train epoch: 23, train loss: 153.9495289325714\n",
      "train epoch: 24, train loss: 153.3985435962677\n",
      "train epoch: 25, train loss: 151.83031558990479\n",
      "train epoch: 26, train loss: 152.19201147556305\n",
      "train epoch: 27, train loss: 151.88468992710114\n",
      "train epoch: 28, train loss: 150.6076455116272\n",
      "train epoch: 29, train loss: 151.41813457012177\n",
      "train epoch: 30, train loss: 150.82554519176483\n",
      "------test------\n",
      "train epoch: 31, train loss: 149.53355491161346\n",
      "train epoch: 32, train loss: 149.2834836244583\n",
      "train epoch: 33, train loss: 150.09891986846924\n",
      "train epoch: 34, train loss: 147.92123436927795\n",
      "train epoch: 35, train loss: 147.74122834205627\n",
      "train epoch: 36, train loss: 147.52955734729767\n",
      "train epoch: 37, train loss: 148.4888792037964\n",
      "train epoch: 38, train loss: 148.84737133979797\n",
      "train epoch: 39, train loss: 147.6687788963318\n",
      "train epoch: 40, train loss: 147.5464105606079\n",
      "------test------\n",
      "train epoch: 41, train loss: 146.1954116821289\n",
      "train epoch: 42, train loss: 146.451935172081\n",
      "train epoch: 43, train loss: 145.47064900398254\n",
      "train epoch: 44, train loss: 145.8670049905777\n",
      "train epoch: 45, train loss: 144.94547247886658\n",
      "train epoch: 46, train loss: 145.93550562858582\n",
      "train epoch: 47, train loss: 145.54215967655182\n",
      "train epoch: 48, train loss: 145.81163227558136\n",
      "train epoch: 49, train loss: 143.94515752792358\n",
      "train epoch: 50, train loss: 144.88811790943146\n",
      "------test------\n",
      "train epoch: 51, train loss: 145.7887887954712\n",
      "train epoch: 52, train loss: 146.9670832157135\n",
      "train epoch: 53, train loss: 145.5402672290802\n",
      "train epoch: 54, train loss: 145.07153630256653\n",
      "train epoch: 55, train loss: 144.59235966205597\n",
      "train epoch: 56, train loss: 146.34764194488525\n",
      "train epoch: 57, train loss: 144.6198672056198\n",
      "train epoch: 58, train loss: 143.01109313964844\n",
      "train epoch: 59, train loss: 142.54924654960632\n",
      "train epoch: 60, train loss: 142.21054315567017\n",
      "------test------\n",
      "train epoch: 61, train loss: 143.15834641456604\n",
      "train epoch: 62, train loss: 144.20941472053528\n",
      "train epoch: 63, train loss: 142.70831644535065\n",
      "train epoch: 64, train loss: 141.35824143886566\n",
      "train epoch: 65, train loss: 141.55861735343933\n",
      "train epoch: 66, train loss: 142.08824932575226\n",
      "train epoch: 67, train loss: 142.2721892595291\n",
      "train epoch: 68, train loss: 141.22062921524048\n",
      "train epoch: 69, train loss: 141.33402705192566\n",
      "train epoch: 70, train loss: 141.45906269550323\n",
      "------test------\n",
      "train epoch: 71, train loss: 141.36838912963867\n",
      "train epoch: 72, train loss: 146.1687982082367\n",
      "train epoch: 73, train loss: 143.74923706054688\n",
      "train epoch: 74, train loss: 140.72808623313904\n",
      "train epoch: 75, train loss: 142.10812747478485\n",
      "train epoch: 76, train loss: 141.06033742427826\n",
      "train epoch: 77, train loss: 139.81348276138306\n",
      "train epoch: 78, train loss: 141.2363052368164\n",
      "train epoch: 79, train loss: 140.3772383928299\n",
      "train epoch: 80, train loss: 139.009490609169\n",
      "------test------\n",
      "train epoch: 81, train loss: 142.68286526203156\n",
      "train epoch: 82, train loss: 143.95193755626678\n",
      "train epoch: 83, train loss: 141.71218872070312\n",
      "train epoch: 84, train loss: 139.81522619724274\n",
      "train epoch: 85, train loss: 139.70215618610382\n",
      "train epoch: 86, train loss: 139.87144827842712\n",
      "train epoch: 87, train loss: 140.2745097875595\n",
      "train epoch: 88, train loss: 141.01800322532654\n",
      "train epoch: 89, train loss: 143.43693852424622\n",
      "train epoch: 90, train loss: 145.62194049358368\n",
      "------test------\n",
      "train epoch: 91, train loss: 143.88929307460785\n",
      "train epoch: 92, train loss: 139.71076905727386\n",
      "train epoch: 93, train loss: 139.35644924640656\n",
      "train epoch: 94, train loss: 137.88487017154694\n",
      "train epoch: 95, train loss: 147.4151313304901\n",
      "train epoch: 96, train loss: 147.44654297828674\n",
      "train epoch: 97, train loss: 140.5910700559616\n",
      "train epoch: 98, train loss: 138.95344734191895\n",
      "train epoch: 99, train loss: 138.64094018936157\n",
      "train epoch: 100, train loss: 141.19039475917816\n",
      "------test------\n",
      "train epoch: 101, train loss: 139.70248401165009\n",
      "train epoch: 102, train loss: 139.15660977363586\n",
      "train epoch: 103, train loss: 139.06650245189667\n",
      "train epoch: 104, train loss: 138.06673884391785\n",
      "train epoch: 105, train loss: 139.29660618305206\n",
      "train epoch: 106, train loss: 139.39489424228668\n",
      "train epoch: 107, train loss: 137.8251314163208\n",
      "train epoch: 108, train loss: 137.56875693798065\n",
      "train epoch: 109, train loss: 145.93102037906647\n",
      "train epoch: 110, train loss: 160.49426794052124\n",
      "------test------\n",
      "train epoch: 111, train loss: 148.64221215248108\n",
      "train epoch: 112, train loss: 141.1624252796173\n",
      "train epoch: 113, train loss: 139.25452530384064\n",
      "train epoch: 114, train loss: 138.5263353586197\n",
      "train epoch: 115, train loss: 138.42298090457916\n",
      "train epoch: 116, train loss: 139.33458924293518\n",
      "train epoch: 117, train loss: 139.07918334007263\n",
      "train epoch: 118, train loss: 138.27447593212128\n",
      "train epoch: 119, train loss: 138.84542882442474\n",
      "train epoch: 120, train loss: 138.73542845249176\n",
      "------test------\n",
      "train epoch: 121, train loss: 138.8472616672516\n",
      "train epoch: 122, train loss: 143.24146509170532\n",
      "train epoch: 123, train loss: 141.51820540428162\n",
      "train epoch: 124, train loss: 138.84086906909943\n",
      "train epoch: 125, train loss: 138.22674751281738\n",
      "train epoch: 126, train loss: 139.54219222068787\n",
      "train epoch: 127, train loss: 139.1720473766327\n",
      "train epoch: 128, train loss: 138.24810087680817\n",
      "train epoch: 129, train loss: 137.9062980413437\n",
      "train epoch: 130, train loss: 138.65776360034943\n",
      "------test------\n",
      "train epoch: 131, train loss: 152.9964804649353\n",
      "train epoch: 132, train loss: 149.0069478750229\n",
      "train epoch: 133, train loss: 143.79514753818512\n",
      "train epoch: 134, train loss: 140.03756642341614\n",
      "train epoch: 135, train loss: 138.23682069778442\n",
      "train epoch: 136, train loss: 137.70081102848053\n",
      "train epoch: 137, train loss: 141.47907495498657\n",
      "train epoch: 138, train loss: 139.198437333107\n",
      "train epoch: 139, train loss: 138.57491314411163\n",
      "train epoch: 140, train loss: 139.85660672187805\n",
      "------test------\n",
      "train epoch: 141, train loss: 138.8700157403946\n",
      "train epoch: 142, train loss: 138.95440101623535\n",
      "train epoch: 143, train loss: 137.6931667327881\n",
      "train epoch: 144, train loss: 138.1959617137909\n",
      "train epoch: 145, train loss: 137.26966083049774\n",
      "train epoch: 146, train loss: 136.82782649993896\n",
      "train epoch: 147, train loss: 137.72153854370117\n",
      "train epoch: 148, train loss: 158.9062660932541\n",
      "train epoch: 149, train loss: 149.9880850315094\n",
      "train epoch: 150, train loss: 143.041729927063\n",
      "------test------\n",
      "train epoch: 151, train loss: 140.40815830230713\n",
      "train epoch: 152, train loss: 137.82934844493866\n",
      "train epoch: 153, train loss: 137.63484358787537\n",
      "train epoch: 154, train loss: 137.1551752090454\n",
      "train epoch: 155, train loss: 137.57649064064026\n",
      "train epoch: 156, train loss: 137.46808469295502\n",
      "train epoch: 157, train loss: 137.85630464553833\n",
      "train epoch: 158, train loss: 149.58886802196503\n",
      "train epoch: 159, train loss: 149.20136332511902\n",
      "train epoch: 160, train loss: 141.95632934570312\n",
      "------test------\n",
      "train epoch: 161, train loss: 138.72089982032776\n",
      "train epoch: 162, train loss: 137.6352608203888\n",
      "train epoch: 163, train loss: 137.52569258213043\n",
      "train epoch: 164, train loss: 137.5984102487564\n",
      "train epoch: 165, train loss: 138.01120221614838\n",
      "train epoch: 166, train loss: 138.79211246967316\n",
      "train epoch: 167, train loss: 137.8445029258728\n",
      "train epoch: 168, train loss: 144.7698324918747\n",
      "train epoch: 169, train loss: 142.65088737010956\n",
      "train epoch: 170, train loss: 139.8569494485855\n",
      "------test------\n",
      "train epoch: 171, train loss: 138.937309384346\n",
      "train epoch: 172, train loss: 139.5803689956665\n",
      "train epoch: 173, train loss: 137.76004314422607\n",
      "train epoch: 174, train loss: 137.04454898834229\n",
      "train epoch: 175, train loss: 141.2951499223709\n",
      "train epoch: 176, train loss: 140.47420024871826\n",
      "train epoch: 177, train loss: 137.2355818748474\n",
      "train epoch: 178, train loss: 136.62713766098022\n",
      "train epoch: 179, train loss: 136.65303587913513\n",
      "train epoch: 180, train loss: 137.04407215118408\n",
      "------test------\n",
      "train epoch: 181, train loss: 137.44880259037018\n",
      "train epoch: 182, train loss: 136.63235306739807\n",
      "train epoch: 183, train loss: 136.57847046852112\n",
      "train epoch: 184, train loss: 136.30084693431854\n",
      "train epoch: 185, train loss: 136.70489192008972\n",
      "train epoch: 186, train loss: 137.15937733650208\n",
      "train epoch: 187, train loss: 140.72850346565247\n",
      "train epoch: 188, train loss: 138.1605714559555\n",
      "train epoch: 189, train loss: 138.3495032787323\n",
      "train epoch: 190, train loss: 137.0590776205063\n",
      "------test------\n",
      "train epoch: 191, train loss: 137.76634633541107\n",
      "train epoch: 192, train loss: 136.341392993927\n",
      "train epoch: 193, train loss: 137.068510055542\n",
      "train epoch: 194, train loss: 163.58008980751038\n",
      "train epoch: 195, train loss: 152.89144217967987\n",
      "train epoch: 196, train loss: 144.20726895332336\n",
      "train epoch: 197, train loss: 141.27711951732635\n",
      "train epoch: 198, train loss: 138.8043761253357\n",
      "train epoch: 199, train loss: 138.47582042217255\n",
      "train epoch: 200, train loss: 137.45218515396118\n",
      "------test------\n",
      "train epoch: 201, train loss: 137.2593194246292\n",
      "train epoch: 202, train loss: 136.85309886932373\n",
      "train epoch: 203, train loss: 142.49210059642792\n",
      "train epoch: 204, train loss: 138.72185349464417\n",
      "train epoch: 205, train loss: 136.6555392742157\n",
      "train epoch: 206, train loss: 139.58606123924255\n",
      "train epoch: 207, train loss: 139.23123478889465\n",
      "train epoch: 208, train loss: 137.86165416240692\n",
      "train epoch: 209, train loss: 138.6478990316391\n",
      "train epoch: 210, train loss: 137.25928962230682\n",
      "------test------\n",
      "train epoch: 211, train loss: 137.21302151679993\n",
      "train epoch: 212, train loss: 137.87226378917694\n",
      "train epoch: 213, train loss: 136.89599931240082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e86705866490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# print(index_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimage_batch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/person-following-drl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/person-following-drl/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3e28c6e7fecf>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(train_epoches):\n",
    "    train_loss_epoch = []\n",
    "    for image_batch, index_batch in train_dataset_loader:\n",
    "        # print(index_batch)\n",
    "        image_batch_tensor = image_batch.clone().detach().float().to(device)\n",
    "        encoded_data = encoder(image_batch_tensor)\n",
    "        # print(encoded_data.shape)\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # print(decoded_data.shape)\n",
    "        # loss = loss_fn(decoded_data, image_batch_tensor)\n",
    "        loss = torch.sqrt((decoded_data - image_batch_tensor).pow(2).mean())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_numpy = loss.detach().cpu().numpy()\n",
    "        # print(loss_numpy)\n",
    "        train_loss_epoch.append(loss_numpy)\n",
    "    if (i % test_frequency == 0 and i != 0):\n",
    "        print('------test------')\n",
    "        selected_sample_index = random.randint(0, len(myDataset))\n",
    "        selected_sample = myDataset[selected_sample_index][0]\n",
    "        selected_image_origin = np.expand_dims(selected_sample, 0)\n",
    "        # print(selected_image_origin.shape)\n",
    "        selected_image_origin = torch.tensor(selected_image_origin, dtype=torch.float)\n",
    "        selected_image_origin_tensor = selected_image_origin.clone().detach().to(device)\n",
    "        selected_image_infer_tensor = decoder(encoder(selected_image_origin_tensor))\n",
    "        selected_image_infer = selected_image_infer_tensor.detach().cpu().numpy()[0]\n",
    "        selected_image_origin_show =  selected_sample[0] * 255.0\n",
    "        selected_image_infer_show = selected_image_infer[0] * 255.0\n",
    "        image_concat = np.hstack((selected_image_origin_show, selected_image_infer_show))\n",
    "        cv2.imwrite(\"result.png\", image_concat)\n",
    "    if (i % save_weight_frequency == 0 and i != 0):\n",
    "        torch.save(encoder.state_dict(), \"./weights/{}_encoder.pth\".format(i))\n",
    "        torch.save(decoder.state_dict(), \"./weights/{}_decoder.pth\".format(i))\n",
    "    train_loss_avg = np.mean(train_loss_epoch)\n",
    "    writer.add_scalar('loss', train_loss_avg * 1000, global_step=i+1)\n",
    "    print(\"train epoch: {}, train loss: {}\".format(i+1, train_loss_avg * 1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1ab5697f9e69cd78b920c03fde1b23dee35d76387e0a20d35fdb77766e38d53b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
